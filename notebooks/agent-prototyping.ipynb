{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39cfd2b4-19d6-4fc4-8983-2765870fb716",
   "metadata": {},
   "source": [
    "# Prototyping the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b3dfea-d87e-4189-92a5-76cba6684b9c",
   "metadata": {},
   "source": [
    "## Section 1: Getting Started with Llama Stack\n",
    "\n",
    "This notebook will help you set up your environment for this tutorial. Specifically, we will cover installing the necessary libraries, configuring essential parameters, and connecting to a Llama Stack server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c13294b-e1ea-4d89-9c98-dd106e638e46",
   "metadata": {},
   "source": [
    "### Installing Dependencies\n",
    "\n",
    "This code requires `llama-stack` and the `llama-stack-client` python packages. Let's begin by installing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd337c60-90db-4145-b8d2-e8d69d30b4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qr requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afcf23d-c587-4346-a09b-55bb9d4771b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from kubernetes.client.api_client import ApiClient\n",
    "from kubernetes.client.rest import ApiException\n",
    "from kubernetes.client import CoreV1Api\n",
    "from kubernetes.config import load_incluster_config\n",
    "from llama_stack_client import Agent, LlamaStackClient\n",
    "from llama_stack_client.lib.agents.client_tool import client_tool\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "from llama_stack_client.lib.agents.react.agent import ReActAgent\n",
    "from llama_stack_client.lib.agents.react.tool_parser import ReActOutput\n",
    "from llama_stack_client.types import UserMessage\n",
    "from rich import print\n",
    "from termcolor import cprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b97272-31cf-4a65-81b0-2e62fce899c7",
   "metadata": {},
   "source": [
    "#### Setting the Environment Variables\n",
    "\n",
    "We will symlink the [`env`](env) file to create a new file called `.env`. We've included as many reasonable defaults as possible to get you started, but please use this file to make any customizations needed for your environment such as the the location of the Llama Stack server endpoint..\n",
    "\n",
    "##### Environment variables required for all sections\n",
    "- `REMOTE_BASE_URL`: the URL of the remote Llama Stack server.\n",
    "- `LLM_MODEL_ID`: the ID of the used LLM.\n",
    "- `TEMPERATURE` (optional): the temperature to use during inference. Defaults to 0.0.\n",
    "- `MAX_TOKENS` (optional): the maximum number of tokens that can be generated in the completion. Defaults to 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e664a86-cf0e-43f5-b195-0bc53d45386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b46847-3d66-4d26-9912-9f136b5b8f98",
   "metadata": {},
   "source": [
    "### Setting Up the Server Connection\n",
    "\n",
    "Establish the connection to your Llama Stack server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d6b638-67d8-4487-9607-28854540b916",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = environ.get('LLAMA_STACK_URL')\n",
    "model_id = environ.get('LLM_MODEL_ID')\n",
    "\n",
    "client = LlamaStackClient(base_url=base_url)\n",
    "\n",
    "print(f'Connected to Llama Stack server at {base_url}')\n",
    "print('Registered models:')\n",
    "print(client.models.list())\n",
    "print(f'Using model: {model_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f225c6d-0b94-4763-893f-a4dfdfcb2a75",
   "metadata": {},
   "source": [
    "### Initializing the Inference Parameters\n",
    "\n",
    "Fetch the inference-related parameters from the corresponding environment variables and convert them to the format Llama Stack expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5feb363-2a38-48df-bf46-8bad1aae8f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = float(environ.get(\"TEMPERATURE\", 0.0))\n",
    "strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = int(environ.get(\"MAX_TOKENS\", 4096))\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "print(f'sampling parameters: {sampling_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5f66df-c7a6-4b91-97ef-8d74fa9c5623",
   "metadata": {},
   "source": [
    "Now, let's use the Llama stack inference API to greet our LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d02115-c749-48a0-be97-d8db867607c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = UserMessage(\n",
    "    content=\"Hi, how are you?\",\n",
    "    role=\"user\",\n",
    ")\n",
    "client.inference.chat_completion(\n",
    "    model_id=model_id,\n",
    "    messages=[message],\n",
    "    sampling_params=sampling_params,\n",
    ").completion_message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211bbac0-1272-4514-9883-099dd753a5b0",
   "metadata": {},
   "source": [
    "Now that we've connected to Llama Stack, let's get started building the agentic AI system!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8162c144-0dc7-42e5-b7bd-10ef31405453",
   "metadata": {},
   "source": [
    "## Section 2: Simple Agent with Tool Calling\n",
    "\n",
    "This section covers how to build a simple agent using Llama Stack's agent framework, enhanced with a single tool: the builtin web search tool. This capability will  allow the agent to retrieve up to date external information beyond the limits of its training data. This is an important step toward developing a more capable and autonomous agent.\n",
    "\n",
    "### Overview\n",
    "\n",
    "This tutorial walks you through how to build your own AI agent who can search the web:\n",
    "\n",
    "1. Configure a Llama Stack agent.\n",
    "2. Enhance the agent by providing it access to a specific tool\n",
    "2. Interact with the agent and tests its use of the web search tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fad660-a71a-4789-9d43-5c36c3e98c51",
   "metadata": {},
   "source": [
    "### Using a built-in tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c610ddd-ab25-4241-a389-340bd356ad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "    You are a helpful assistant. \n",
    "    When a user asks a question, you MUST use the websearch tool.\n",
    "\"\"\" \n",
    "\n",
    "websearch_agent = Agent(\n",
    "    client, \n",
    "    model=model_id,\n",
    "    instructions=instructions,\n",
    "    tools=['builtin::websearch'],\n",
    "    sampling_params=sampling_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71003222-900e-4aa9-88d1-8b4bbe82a108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_session(agent, session_name, user_prompts):\n",
    "    session_id = agent.create_session(session_name)\n",
    "    print(f'Created new session {session_name}')\n",
    "    print(f'Looping over user prompts: {user_prompts}')\n",
    "    for prompt in user_prompts:\n",
    "        print(\"\\n\"+\"=\"*50)\n",
    "        cprint(f\"Processing user query: {prompt}\", \"blue\")\n",
    "        print(\"=\"*50)\n",
    "        response = agent.create_turn(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            session_id=session_id,\n",
    "            stream='True'\n",
    "        )\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853b3b40-d7af-4033-98d9-42d7daa888f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompts = [\"What is the latest OpenShift version?\"]\n",
    "run_session(websearch_agent, 'websearch-test-session', user_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02f56ec-b107-49d5-9c9b-5d91164d2c46",
   "metadata": {},
   "source": [
    "### Creating a custom tool function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad98714-b21f-4990-a36d-b11a29fd0983",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_incluster_config()\n",
    "api_instance = CoreV1Api(ApiClient())\n",
    "\n",
    "\n",
    "def get_pod_log_test(pod_name: str, namespace: str, container_name: str):\n",
    "    \"\"\"\n",
    "    Provide the location upon request.\n",
    "\n",
    "    :param pod_name: The name of the target pod\n",
    "    :param namespace: The name of the target namespace\n",
    "    :param container_name: The name of the target container within the target pod\n",
    "    :returns: Logs of the target pod\n",
    "    \"\"\"\n",
    "    try:\n",
    "        api_response = api_instance.read_namespaced_pod_log(\n",
    "            pod_name, namespace, container=container_name, tail_lines=100\n",
    "        )\n",
    "        print(api_response)\n",
    "    except ApiException as e:\n",
    "        print(\"Exception when calling CoreV1Api->read_namespaced_pod_log: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec26fb09-9c2f-4152-a49a-9f422e92775d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pod_name = 'java-app-build-run-bad-x2ldw-build-pod-retry5' # insert the name of the failed build pod\n",
    "namespace = 'demo-pipeline'\n",
    "container_name = 'step-s2i-build'\n",
    "\n",
    "get_pod_log_test(pod_name, namespace, container_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9e1d46-c61c-44fc-ad4f-f85cf612900f",
   "metadata": {},
   "source": [
    "### Configure an agent for tool use.\n",
    "\n",
    "- **Agent Initialization**: First we create an `Agent` instance with the desired LLM model, agent instructions and tools.\n",
    "\n",
    "- **Instructions**: The `instructions` parameter, also referred to as the system prompt, specifies the agent's role and behavior. In this example, the agent is configured as a helpful web search assistant. It is instructed to use a tool whenever a web search is required and to respond in a friendly and helpful tone.\n",
    "\n",
    "- **Tools**: The `tools` parameter defines the tools available to the agent. In this case, the `get_pod_log` tool is used, which enables the agent to look up logs from a pod.\n",
    "\n",
    "- **How It Works**: When a user query is provided, the agent processes the input and determines whether a tool is required to fulfill the request. If the query involves retrieving logs from a pod, the agent invokes the `get_pod_log` tool. The tool interacts with the Kubernetes API server to fetch the container logs. This workflow ensures that the agent can handle a wide range of queries effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458adf3d-9c8f-4c8e-b667-adcb6beff0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@client_tool\n",
    "def get_pod_log(pod_name: str, namespace: str, container_name: str):\n",
    "    \"\"\"\n",
    "    Provide the location upon request.\n",
    "\n",
    "    :param pod_name: The name of the target pod\n",
    "    :param namespace: The name of the target namespace\n",
    "    :param container_name: The name of the target container within the target pod\n",
    "    :returns: Logs of the target pod\n",
    "    \"\"\"\n",
    "    try:\n",
    "        api_response = api_instance.read_namespaced_pod_log(\n",
    "            pod_name, namespace, container=container_name, tail_lines=100\n",
    "        )\n",
    "        return api_response\n",
    "    except ApiException as e:\n",
    "        print(\"Exception when calling CoreV1Api->read_namespaced_pod_log: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb74444-e0a6-4822-b8c9-f437b74769e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "    You are a helpful assistant. \n",
    "    When a user asks for pod logs, you MUST use the get_pod_log tool.\n",
    "\"\"\" \n",
    "\n",
    "custom_tool_agent = Agent(\n",
    "    client, \n",
    "    model=model_id,\n",
    "    instructions=instructions,\n",
    "    tools=[get_pod_log],\n",
    "    sampling_params=sampling_params\n",
    ")\n",
    "\n",
    "\n",
    "user_prompts = [\n",
    "    f\"Analyze the logs of the '{container_name}' container within the '{pod_name}' pod in namespace '{namespace}'.\"\n",
    "    f\"Summarize the logs in 3 bullet points.\"\n",
    "]\n",
    "run_session(custom_tool_agent, 'tool-test-session', user_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2575bc3-ebb2-4d1d-9b9e-91ef0e066a9d",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "- We've demonstrated how to set up Llama Stack agents and extended them with builtin tools (like web search) that come prepackaged with Llama Stack.\n",
    "- We've shown that this simple approach can provide significantly increased functionality of existing open source LLM's. \n",
    "- This will serves as a foundational example for the more advanced examples to come involving Agentic RAG, External Tools, and complex agentic patterns.\n",
    "\n",
    "Continue to the section to learn how we can upgrade our agents to solve even more complex and multi-step tasks using advanced agentic patterns. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d033b7-2ad1-41d3-bc30-8ed856dd4210",
   "metadata": {},
   "source": [
    "## Section 3: Prompt Chaining\n",
    "\n",
    "Building on the simple agent introduced in section 2, this tutorial continues the agent-focused section of our series by introducing techniques that make the agent smarter and more autonomous: **Prompt Chaining** and the **ReAct (Reasoning + Acting) framework**. These approaches allow the agent to complete multi-step tasks, dynamically choose tools, and adjust its behavior based on context.\n",
    "\n",
    "- **Prompt Chaining** connects multiple prompts into a coherent sequence, allowing the agent to maintain context and perform multi-step reasoning across tool invocations. \n",
    "- **ReAct Agent** combines reasoning and acting steps in a loop, enabling the agent to make decisions, use tools dynamically, and adapt based on intermediate results. \n",
    "\n",
    "### Overview\n",
    "\n",
    "In this notebook, you'll explore three agent configurations:\n",
    "1. **Simple Agent (Baseline)** – Uses a single web search tool.\n",
    "2. **Prompt Chaining** – Performs structured, multi-step reasoning by chaining prompts and responses.\n",
    "3. **ReAct Agent** – Dynamically plans and executes actions using a loop of reasoning and tool use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8feee7-57e2-4a51-93e2-845c40d9601a",
   "metadata": {},
   "source": [
    "### Prompt chaining with websearch tool and client tool\n",
    "\n",
    "In this section, we demonstrate a more sophisticated use case that combines the use of two tools: pod log extraction and web search.\n",
    "\n",
    "1. **Pod Log Extraction**: Use the `get_pod_log` client tool from the previous section to read the logs of a given pod.\n",
    "2. **Contextual Search**: Leverage the detected location to formulate the correct websearch query.\n",
    "\n",
    "For example, when a user asks \"Why is my application pod failing?\", the agent will:\n",
    "- First read the pod logs using `get_pod_log`.\n",
    "- Then use these logs to search for debugging information.\n",
    "- Finally, present a comprehensive response.\n",
    "\n",
    "This demonstrates how the builtin websearch tool and custom client tools can work together to provide intelligent, context-aware responses without requiring explicit logging input from the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de49c988-ee05-4792-8d2a-a5154b44934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "    You are a helpful assistant.\n",
    "    When a user asks about logs from a pod, you MUST use the get_pod_log tool.\n",
    "\"\"\"\n",
    "\n",
    "prompt_chain_agent = Agent(\n",
    "    client, \n",
    "    model=model_id,\n",
    "    instructions=instructions,\n",
    "    tools=[get_pod_log, 'builtin::websearch'],\n",
    "    sampling_params=sampling_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a75df4-2ee7-4af1-aadb-a5add10eb397",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompts = [\n",
    "    f\"Retrieve the logs of the {container_name} container within the {pod_name} pod in namespace {namespace}.\",\n",
    "    \"Look up relevant troubleshooting information from the web.\",\n",
    "    \"Provide a short summary and troubleshooting recommendations in bullet points.\"\n",
    "]\n",
    "run_session(prompt_chain_agent, 'prompt-chaining-session', user_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff8d26a-99c5-487a-9bf9-f6f3b8c17053",
   "metadata": {},
   "source": [
    "## Section 4: ReAct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de59bce3-21e6-4fe5-9fb4-96d0f98cedd9",
   "metadata": {},
   "source": [
    "### ReAct Agent with websearch tool and client tool\n",
    "\n",
    "This section demonstrates the ReAct (Reasoning and Acting) framework in action.\n",
    "\n",
    "Here is a walkthrough of how the ReAct agent will tackle this same \"troubleshoot pod\" problem:\n",
    "\n",
    "When asked \"Are there any published troubleshooting guides for my application log?\", the agent will:\n",
    "\n",
    "1. **Reason** that it needs to get the pod logs first.\n",
    "2. **Act** by calling the `get_pod_log` client tool.\n",
    "3. **Observe** the log result.\n",
    "4. **Reason** that it now needs to search for troubleshooting guides.\n",
    "5. **Act** by calling the `websearch` tool with observed pod logs.\n",
    "6. **Observe** and processes the search results into a final answer. \n",
    "\n",
    "Unlike prompt chaining which follows fixed steps, ReAct dynamically breaks down tasks and adapts its approach based on the results of each step. This makes it more flexible and capable of handling complex, real-world queries effectively.\n",
    "\n",
    "Reasoning models output their \\<thought\\> process first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a47f6c5-e756-4ff3-bcb1-5f80e2a7417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_react_agent = ReActAgent(\n",
    "    client=client,\n",
    "    model=model_id,\n",
    "    tools=[get_pod_log, \"builtin::websearch\"],\n",
    "    response_format={\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": ReActOutput.model_json_schema(),\n",
    "    },\n",
    "    sampling_params=sampling_params,\n",
    ")\n",
    "\n",
    "user_prompts = [\n",
    "    f\"There's a pod {pod_name} in namespace {namespace} with a container {container_name}\"\n",
    "    f\" that's been failing. Read its logs and do a web search to find out why it's been\"\n",
    "    f\" failing and provide a troubleshooting recommendation based your findings.\"\n",
    "]\n",
    "run_session(simple_react_agent, 'react-session', user_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e77c3e7-dc1d-4e42-900b-c3349cbf48ce",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "- This section demonstrated how to build more capable agents using Prompt Chaining and the ReAct framework.\n",
    "- It showed how agents can maintain context across multiple steps and perform structured, multi-step reasoning.\n",
    "- It highlights how ReAct enables dynamic tool selection and adaptive decision-making based on intermediate results.\n",
    "- These techniques enhance agent autonomy and make them more suitable for complex operational tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af241950-55cd-40b9-99e1-6fd0e1168d83",
   "metadata": {},
   "source": [
    "## Section 5: MCP Tools\n",
    "\n",
    "Let's now look at more advanced use cases for agents where a single tool call is insufficient to complete the required task.\n",
    "\n",
    "We will also use [MCP tools](https://github.com/modelcontextprotocol/servers) (which can be deployed onto an OpenShift cluster) throughout this section to show users how to extend their agents beyond Llama Stacks's current builtin tools and connect to many different services and data sources to build their own custom agents.  \n",
    "\n",
    "### Agent Example:\n",
    "\n",
    "This notebook will walkthrough how to build a system that can answer the following question via an agent built with Llama Stack:\n",
    "\n",
    "- *\"Review OpenShift logs for the failing pod. If you find an error, search for a solution. Create a Github issue with your findings.\"*\n",
    "\n",
    "### MCP Tools:\n",
    "\n",
    "#### OpenShift MCP Server\n",
    "\n",
    "Throughout this notebook we will be relying on the [Kubernetes MCP server](https://github.com/manusa/kubernetes-mcp-server) by [manusa](https://github.com/manusa) to interact with our OpenShift cluster.\n",
    "\n",
    "#### Github MCP Server\n",
    "\n",
    "Throughout this notebook we will be relying on the [Github MCP server](https://github.com/github/github-mcp-server) by Github to interact with our Github repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281e2032-7541-45a7-8761-c8be2d971514",
   "metadata": {},
   "source": [
    "### Validate tools are available in our Llama Stack instance\n",
    "\n",
    "When an instance of Llama Stack is redeployed, it may be the case that the tools will need to be re-registered. Also if a tool is already registered with a Llama Stack instance, trying to register another one with the same `toolgroup_id` will throw you an error.\n",
    "\n",
    "For this reason, it is recommended to validate your tools and toolgroups. The following code will check that both the `builtin::websearch` and `mcp::openshift` tools are correctly registered, and if not it will attempt to register them using their specific endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612debaf-719d-48f7-8185-19a49bb5f949",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "registered_tools = client.tools.list()\n",
    "registered_toolgroups = [t.toolgroup_id for t in registered_tools]\n",
    "if \"mcp::openshift\" not in registered_toolgroups:\n",
    "    client.toolgroups.register(\n",
    "        toolgroup_id=\"mcp::openshift\",\n",
    "        provider_id=\"model-context-protocol\",\n",
    "        mcp_endpoint={\"uri\":lab.ocp_mcp_url},\n",
    "    )\n",
    "\n",
    "print(f\"Your Llama Stack server is registered with the following tool groups @ {set(registered_toolgroups)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85041013-2b39-4040-b837-66486f53707c",
   "metadata": {},
   "source": [
    "Now that we've shown that we can successfully accomplish this multi-step multi-tool task using prompt chaining, let's see if we can give our agent a bit more autonomy to perform the same task but with a single prompt instead of a chain. To do this, we will instantiate a **ReAct agent** (which is included in the llama stack python client by default).The ReAct agent is a variant of the simple agent but with the ability to loop through \"Reason then Act\" iterations, thinking through the problem and then using tools until it determines that it's task has been completed successfully.  \n",
    "\n",
    "Unlike prompt chaining which follows fixed steps, ReAct dynamically breaks down tasks and adapts its approach based on the results of each step. This makes it more flexible and capable of handling complex, real-world queries effectively.\n",
    "\n",
    "Below you will see the slight differences in the agent definition and the prompt used to accomplish our task. Be sure to change the redhat-ai-services to $YOUR_GITHUB_USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762a476e-63f3-43d5-bf7a-14514a958c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_react_agent = ReActAgent(\n",
    "    client=client,\n",
    "    model=model_id,\n",
    "    tools=[\"mcp::openshift\", \"builtin::websearch\", 'mcp::github'],\n",
    "    response_format={\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": ReActOutput.model_json_schema(),\n",
    "    },\n",
    "    sampling_params=sampling_params,\n",
    ")\n",
    "\n",
    "user_prompts = [\n",
    "    \"You are an expert OpenShift administrator. Your task is to analyze pod logs,\"\n",
    "    \" summarize the error, and generate a JSON object to create a GitHub issue for tracking.\"\n",
    "    f\" There's a pod '{pod_name}' in namespace '{namespace}' with a container '{container_name}'\"\n",
    "    f\" that's been failing. Review its logs.\"\n",
    "    \" If the logs indicate an error, do a web search to find out why it's been failing,\"\n",
    "    \" using the logs as the 'query' tool parameter name.\" \n",
    "    \" Create a summary message with the category and explanation of the error,\"\n",
    "    ' create a Github issue using {\"name\":\"create_issue\",\"arguments\":'\n",
    "    ' {\"owner\":\"redhat-ai-services\",\"repo\":\"etx-agentic-ai\",'\n",
    "    ' \"title\":\"Issue with Etx pipeline\",\"body\":\"summary of the error\"}}}.'\n",
    "    ' DO NOT add \"assignees\" or any other optional parameters.'\n",
    "]\n",
    "run_session(full_react_agent, 'mcp-session', user_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21107605-41d7-4f30-927f-129279b5045e",
   "metadata": {},
   "source": [
    "### Output Analysis\n",
    "\n",
    "1. First the LLM generated a tool call for the `pods_log` tool included in the **OpenShift MCP server** and fetched the logs for the specified pod. It might at first confuse the name-value order within the tool parameters, but it iterates to generate a correct prompt.\n",
    "2. The tool successfully retrieved the logs for the pod.\n",
    "3. The LLM  then received the logs from the tool call, along with the original query.\n",
    "4. This context was then passed back to the LLM for the final inference. The inference result provided a summary of the pod logs.\n",
    "5. Next the LLM generates a tool call for the `web_search` tool looking for the top answer to the error.\n",
    "6. A final tool call is executed against the **Github MCP server**--using the format provided within the prompt--in order to document the build failure along with troubleshooting information within a new Github issue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
