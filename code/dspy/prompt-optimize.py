import os

import dspy
import mlflow
from rich.pretty import pprint

mlflow.set_tracking_uri("http://localhost:5500")
mlflow.set_experiment("20_days_dspy")
mlflow.dspy.autolog(
    log_compiles=True,    # Track optimization process
    log_evals=True,       # Track evaluation results
    log_traces_from_compile=True  # Track program traces during optimization
)

# 1. Configure the Language Model
#    Replace 'YOUR_OPENAI_API_KEY' with your actual OpenAI API key.
#    You can also set it as an environment variable: export OPENAI_API_KEY='your_key_here'
#    If using a local LM with Ollama or SGLang, adjust api_base and api_key accordingly.

LLM_URL=os.getenv('LLM_URL', 'http://localhost:8080/v1')
API_KEY=os.getenv('API_KEY', 'fake')
LLM_MODEL=os.getenv('LLM_MODEL', 'openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf')
MAX_TOKENS=os.getenv('MAX_TOKENS', 3000)
TEMPERATURE=os.getenv('TEMPERATURE', 0.2)
dspy.enable_logging()

lm = dspy.LM(model=LLM_MODEL,
             api_base=LLM_URL,
             api_key=API_KEY,
             temperature=TEMPERATURE,
             model_type='chat',
             stream=False)
dspy.configure(lm=lm)
dspy.settings.configure(track_usage=True)

# Ensure LM and RM are configured
# lm = dspy.LM(
#     "openai/gpt-4o-mini",
#     api_key=os.environ.get("OPENAI_API_KEY", "YOUR_OPENAI_API_KEY"),
# )
# dspy.configure(lm=lm)

colbertv2_dev_red_hat = dspy.ColBERTv2(
    url="https://dev-red-rag-colbert-dev-red-rag-colbert.apps.sno.sandbox2010.opentlc.com/api/search"
)
dspy.settings.configure(rm=colbertv2_dev_red_hat)

print("--- Day 17: Few-Shot Optimization ---")

# 1. Define RAG signature (from Day 6)
class RAGQA(dspy.Signature):
    """Answer questions based on the provided context."""

    context: str = dspy.InputField(desc="relevant passages from a knowledge base")
    question: str = dspy.InputField()
    answer: str = dspy.OutputField()


# 2. Define a RAG program (from Day 6)
class BasicRAG(dspy.Module):
    def __init__(self, num_passages=3):
        super().__init__()
        self.retrieve = dspy.Retrieve(k=num_passages)
        self.generate_answer = dspy.ChainOfThought(RAGQA)

    def forward(self, question):
        context = self.retrieve(question).passages
        prediction = self.generate_answer(context=context, question=question)
        return prediction


# 3. Create dummy dataset for RAG (simulating HotPotQA examples)
# In a real scenario, use dspy.datasets.HotPotQA
trainset_rag = [
    dspy.Example(
        question="What is Red Hat OpenShift AI?",
        answer="Red Hat OpenShift AI is an artificial intelligence platform",
    ).with_inputs("question"),
    # dspy.Example(
    #     question="What is the capital of Australia?", answer="Canberra"
    # ).with_inputs("question"),
    # dspy.Example(question="When did World War II end?", answer="1945").with_inputs(
    #     "question"
    # ),
    # dspy.Example(
    #     question="What is the chemical formula for sulfuric acid?", answer="H2SO4"
    # ).with_inputs("question"),
]

devset_rag = [
    dspy.Example(
        question="Can I use NVIDIA MIG with RedHat OpenShift AI?", answer="Yes, you can use NVIDIA MIG with Red Hat OpenShift"
    ).with_inputs("question"),
    # dspy.Example(
    #     question="Who invented the light bulb?", answer="Thomas Edison"
    # ).with_inputs("question"),
]

print("\n--- Step 1: Define a Metric for RAG ---")


# 4. Define a RAG-specific metric (answer close match !)
def answer_exact_match(example, pred, trace=None):
    """Evaluates if the predicted answer exactly matches the gold answer (case-insensitive)."""
    pprint("metric>>> " + example.answer.lower() + ":" + pred.answer.lower())
    return example.answer.lower() in pred.answer.lower()

print(f"Metric function defined: {answer_exact_match.__name__}")

print("\n--- Step 2: Evaluate Baseline RAG Program ---")
rag_program = BasicRAG()
evaluator = dspy.Evaluate(
    devset=devset_rag,
    metric=answer_exact_match,
    num_threads=1,
    display_progress=True,
    display_table=True,
)
print("Evaluating baseline RAG program...")
baseline_score = evaluator(rag_program)
print(f"Baseline RAG Program Score: {baseline_score}")

print("\n--- Step 3: Optimize RAG with BootstrapFewShotWithRandomSearch ---")

# 5. Initialize BootstrapFewShotWithRandomSearch optimizer
#    num_candidate_programs: how many random bootstrapped programs to try.
#    num_threads: parallelism for evaluation during optimization.
from dspy.teleprompt import (
    BootstrapFewShotWithRandomSearch,  # Note: from dspy.teleprompt for optimizers
)

optimizer_config = dict(
    max_bootstrapped_demos=1,  # Self-generate up to 2 demos per predictor
    max_labeled_demos=0,  # Don't use labeled demos for bootstrapping (rely on teacher's trace)
    num_candidate_programs=1,  # Try 3 different random seeds for bootstrapping
    num_threads=1,  # Keep low for local testing
)

# Use a deepcopy of the RAG program to optimize.
# The `teacher` argument can be left as default (the program itself) or a more capable LM.
teleprompter_rag = BootstrapFewShotWithRandomSearch(
    metric=answer_exact_match, **optimizer_config
)

print("Compiling RAG program with BootstrapFewShotWithRandomSearch...")
optimized_rag_program = teleprompter_rag.compile(
    student=rag_program.deepcopy(),
    trainset=trainset_rag,
    valset=devset_rag,  # valset used by random search to pick best program
)

print("\n--- Step 4: Evaluate Optimized RAG Program ---")
print("Evaluating optimized RAG program...")
optimized_rag_score = evaluator(optimized_rag_program)
print(f"Optimized RAG Program Score: {optimized_rag_score}")

# Optional: Save and load the optimized program
optimized_rag_program.save("optimized_rag.json")
print("\nOptimized RAG program saved to optimized_rag.json")

loaded_rag_program = BasicRAG()  # Recreate the class structure
loaded_rag_program.load("optimized_rag.json")
print("Optimized RAG program loaded from optimized_rag.json")

# Verify that demos were loaded (they become part of the program's state)
# This check is illustrative, internal demos structure can vary per module/optimizer.
# The demos attribute may not exist in all DSPy versions, so we'll check for it safely
if hasattr(optimized_rag_program, "demos") and hasattr(loaded_rag_program, "demos"):
    print(f"Number of demos in optimized program: {len(optimized_rag_program.demos)}")
    print(f"Number of demos in loaded program: {len(loaded_rag_program.demos)}")
    assert len(optimized_rag_program.demos) == len(loaded_rag_program.demos)
else:
    print(
        "Demo attribute check skipped - demos structure may vary in this DSPy version"
    )
